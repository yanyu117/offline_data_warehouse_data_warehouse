### 用户行为数据模拟

集群日志生成脚本

```
my-userlog

#!/bin/bash
for i in master slave; do
        echo "========== $i =========="
        ssh $i "cd /home/yiyun/software/applog/; java -jar gmall2020-mock-log-2021-10-10.jar >/dev/null 2>&1 &"
done
```

### 日志采集Flume测试

日志采集Flume启停脚本

```
my-flume

#!/bin/bash
case $1 in
"start"){
        for i in master
        do
                echo " --------启动 $i 采集flume-------"
                ssh $i "nohup /home/yiyun/software/flume/bin/flume-ng agent -n a1 -c /home/yiyun/software/flume/conf/ -f /home/yiyun/flume_conf/file_to_kafka.conf >/dev/null 2>&1 &"
        done
};;
"stop"){
        for i in master
        do
                echo " --------停止 $i 采集flume-------"
                ssh $i "ps -ef | grep file_to_kafka | grep -v grep |awk  '{print \$2}' | xargs -n1 kill -9 "
        done
};;
esac
```

```
(1)启动Zookeeper、Kafka集群
	startZk		my-kafka start
(2)启动master的日志采集Flume
	flume-ng agent -n a1 -c conf/ -f flume_conf/file_to_kafka.conf -Dflume.root.logger=info,console
	my-flume start
(3)启动一个Kafka的Console-Consumer
	kafka-console-consumer.sh --bootstrap-server master:9092 --topic topic_log
(4)生成模拟数据
	my-userlog
(5)观察Kafka消费者是否能消费到数据
```

### 业务数据模拟

数据模拟生成脚本

```
my-data 
#!/bin/bash
target_date=$1
if [[ $target_date =~ ^[0-9]{4}-[0-9]{2}-[0-9]{2}$ ]]; then
        for i in master
        do
                echo "--------$i用户行为数据模拟配置文件修改---------"
                ssh $i "sed -i '4s/.*/mock.date: \"$target_date\"/' /home/yiyun/software/applog/application.yml"
        done

        echo "--------master业务数据模拟配置文件修改---------"
        sed -i "16s/.*/mock.date=$target_date/" /home/yiyun/software/db_log/application.properties
        echo "--------数据生成---------"
                cmd1="cd /home/yiyun/software/applog; java -jar gmall2020-mock-log-2021-10-10.jar >/dev/null 2>&1 &"
                cmd2="cd /home/yiyun/software/db_log; java -jar gmall2020-mock-db-2021-11-14.jar >/dev/null 2>&1 &"

                # 执行两个个命令，并将进程号保存到数组中
                echo "master生成用户日志数据以及业务数据"
                declare -a pid_array=()
                for cmd in "$cmd1" "$cmd2" ; do
                        echo "执行$cmd命令"
                        eval "$cmd "
                        pid_array+=($!)
                done
                echo "等待所有进程结束"
                for pid in "${pid_array[@]}"; do
                        wait "$pid"
                done

                # 输出执行完毕
                echo "数据生成完毕"
else
        echo "输入错误"
fi
```

```
导入数据建表语句
	source gmall.sql
生成业务数据
	java -jar gmall2020-mock-db-2021-11-14.jar
数据模拟生成脚本 （用户日志数据和业务数据）
	my-data 2024-11-12
```

### 采集通道Maxwell

Maxwell启停脚本

```
my-maxwell
#!/bin/bash
MAXWELL_HOME=/home/yiyun/software/maxwell
status_maxwell(){
        result=`ps -ef | grep com.zendesk.maxwell.Maxwell | grep -v grep | wc -l`
        return $result
}

start_maxwell(){
        status_maxwell
        if [[ $? -lt 1 ]]; then
                echo "启动Maxwell"
                $MAXWELL_HOME/bin/maxwell --config $MAXWELL_HOME/config.properties --daemon
        else
                echo "Maxwell正在运行"
        fi
}

stop_maxwell(){
        status_maxwell
                if [[ $? -gt 0 ]]; then
                echo "停止Maxwell"
                ps -ef | grep com.zendesk.maxwell.Maxwell | grep -v grep | awk '{print $2}' | xargs kill -9
        else
                echo "Maxwell未在运行"
        fi
}

case $1 in
        start )
                start_maxwell
        ;;
        stop )
                stop_maxwell
        ;;
        restart )
                stop_maxwell
                start_maxwell
        ;;
esac
```

```
Maxwell的工作原理是实时读取MySQL数据库的二进制日志（Binlog），从中获取变更数据，再将变更 数据以JSON格式发送至Kafka等流处理平台。
(1)启动Zookeeper以及Kafka集群
	startZk		my-kafka start
(2)启动启动Maxwell
	my-maxwell start
(3)启动一个Kafka Console Consumer，消费topic_db数据
	kafka-console-consumer.sh --bootstrap-server master:9092 --topic topic_db
(4)生成模拟数据
	cd /home/yiyun/software/db_log/
	java -jar gmall2020-mock-db-2021-11-14.jar 
(5)观察Kafka消费者是否能消费到数据
```

### 日志消费Flume配置

```
按照规划，该Flume需将Kafka中topic_log的数据发往HDFS。并且对每天产生的用户行为日志进行区分，将不同天的数据发往HDFS不同天的路径。
此处选择KafkaSource、FileChannel、HDFSSink。
编程解决数据漂移问题	打包到flume/lib下

flume-ng agent -n a1 -c conf/ -f flume_conf/kafka_to_hdfs_log.conf -Dflume.root.logger=info,console
my-flume2 start
```



```
用户行为数据:
->数据模拟（my-userlog）->发送到kafka->（my—flume start）->hdfs（my-flume2 start）-> .gz格式的hdfs文件
业务数据：
->业务数据模拟（my-data）
	商品类别表（长时间不更新的表）定时一天一次->全量同步（datax）(每个全量表都需要各自专属的json格式的配置文件)（gen_import_config.py）（gen_import_config.sh）-> (使用datax进行数据同步)(mysql_to_hdfs_full.sh all 2024-11-13)->hdfs
	评价表（经常变更的表）实时->增量同步 mysql->(maxwell)（my-maxwell start）->kafka->flume(my-flume3 start)->hdfs
```

```
python /home/yiyun/software/datax/bin/datax.py -p"-Dtargetdir=/origin_data/gmall/db/activity_info_full/2024-11-13" /home/yiyun/software/datax/job/import/gmall.activity_info.json


flume-ng agent -n a1 -c conf/ -f flume_conf/kafka_to_hdfs_db.conf -Dflume.root.logger=info,console

 hdfs dfs -ls /origin_data/gmall/db | grep _inc | awk '{print$8}' | xargs hadoop fs -rm -r -f
```

